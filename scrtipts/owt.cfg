set_vocab_size(50257)
load_tokenized_train('D:/111enwiki9/gpt2_train.bin')
load_tokenized_test('D:/111enwiki9/gpt2_test.bin')
make_dataset()

MAX_ITERS = 3125000
EVAL_INTERVAL = 1000
EVAL_BATCH_COUNT = 20
TRAIN_CONFIG = 'a6b16f1024'
DROP_CONFIG = 'drop1ch1tail25'
MODEL_DIMS = 'e512tt128d60w1024'
#MODEL_DIMS = 'e512d105w1024'
create_model(MPF_TUNE_FINAL_LAYER, MPF_TUNE_EMBED)

train()
compute_exact_test(781250, 0000)
